<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · Sophon.jl</title><script data-outdated-warner src="assets/warner.js"></script><link rel="canonical" href="https://YichengDWu.github.io/Sophon.jl/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script><link href="assets/indigo.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href>Sophon.jl</a></span></div><form class="docs-search" action="search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="tutorials/discontinuous/">Fitting a nonlinear discontinuous function</a></li><li><a class="tocitem" href="tutorials/poisson/">1D Multi-scale Poisson&#39;s Equation</a></li><li><a class="tocitem" href="tutorials/convection/">1D Convection Equation</a></li><li><a class="tocitem" href="tutorials/helmholtz/">2D Helmholtz Equation</a></li><li><a class="tocitem" href="tutorials/allen_cahn/">Allen-Cahn Equation with Sequential Training</a></li><li><a class="tocitem" href="tutorials/SchrödingerEquation/">Schrödinger Equation: A PDE System with Resampling</a></li><li><a class="tocitem" href="tutorials/L_shape/">Poisson equation over an L-shaped domain</a></li></ul></li><li><a class="tocitem" href="references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/YichengDWu/Sophon.jl/blob/main/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Sophon"><a class="docs-heading-anchor" href="#Sophon">Sophon</a><a id="Sophon-1"></a><a class="docs-heading-anchor-permalink" href="#Sophon" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/YichengDWu/Sophon.jl">Sophon</a>.</p><ul><li><a href="#Sophon.AdaptiveTraining"><code>Sophon.AdaptiveTraining</code></a></li><li><a href="#Sophon.ChainState"><code>Sophon.ChainState</code></a></li><li><a href="#Sophon.ConstantFunction"><code>Sophon.ConstantFunction</code></a></li><li><a href="#Sophon.DeepONet"><code>Sophon.DeepONet</code></a></li><li><a href="#Sophon.DiscreteFourierFeature"><code>Sophon.DiscreteFourierFeature</code></a></li><li><a href="#Sophon.FactorizedDense"><code>Sophon.FactorizedDense</code></a></li><li><a href="#Sophon.FourierFeature"><code>Sophon.FourierFeature</code></a></li><li><a href="#Sophon.NonAdaptiveTraining"><code>Sophon.NonAdaptiveTraining</code></a></li><li><a href="#Sophon.PINN"><code>Sophon.PINN</code></a></li><li><a href="#Sophon.PINNAttention"><code>Sophon.PINNAttention</code></a></li><li><a href="#Sophon.QuasiRandomSampler"><code>Sophon.QuasiRandomSampler</code></a></li><li><a href="#Sophon.RBF"><code>Sophon.RBF</code></a></li><li><a href="#Sophon.ScalarLayer"><code>Sophon.ScalarLayer</code></a></li><li><a href="#Sophon.SplitFunction"><code>Sophon.SplitFunction</code></a></li><li><a href="#Sophon.TriplewiseFusion"><code>Sophon.TriplewiseFusion</code></a></li><li><a href="#Sophon.BACON-Tuple{Int64, Int64, Int64, Real}"><code>Sophon.BACON</code></a></li><li><a href="#Sophon.FourierAttention-Tuple{Int64, Int64, Function, Any}"><code>Sophon.FourierAttention</code></a></li><li><a href="#Sophon.FourierFilterNet-Tuple{Int64, Int64}"><code>Sophon.FourierFilterNet</code></a></li><li><a href="#Sophon.FourierNet-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function, Tuple{Vararg{T, N}} where {N, T}}} where {N, T&lt;:Int64}"><code>Sophon.FourierNet</code></a></li><li><a href="#Sophon.FullyConnected-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function}} where {N, T&lt;:Int64}"><code>Sophon.FullyConnected</code></a></li><li><a href="#Sophon.Sine-Union{Tuple{Pair{T, T}}, Tuple{T}} where T&lt;:Int64"><code>Sophon.Sine</code></a></li><li><a href="#Sophon.Siren-Tuple{Int64, Int64}"><code>Sophon.Siren</code></a></li><li><a href="#Sophon.discretize-Tuple{Any, PINN, Sophon.PINNSampler, Sophon.AbstractTrainingAlg}"><code>Sophon.discretize</code></a></li><li><a href="#Sophon.gaussian"><code>Sophon.gaussian</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="Sophon.AdaptiveTraining" href="#Sophon.AdaptiveTraining"><code>Sophon.AdaptiveTraining</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaptiveTraining(pde_weights, bcs_weights)</code></pre><p>Adaptive weights for the loss functions. Here <code>pde_weights</code> and <code>bcs_weights</code> are functions that take in <code>(phi, x, θ)</code> and return the point-wise weights. Note that <code>bcs_weights</code> can be real numbers but they will be converted to functions that return the same numbers.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/compact/NeuralPDE/training_strategies.jl#L54-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.ChainState" href="#Sophon.ChainState"><code>Sophon.ChainState</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ChainState(model, rng::AbstractRNG=Random.default_rng())</code></pre><p>It this similar to <code>Lux.Chain</code> but wraps it in a stateful container.</p><p><strong>Fields</strong></p><ul><li><code>model</code>: The neural network.</li><li><code>states</code>: The states of the neural network.</li></ul><p><strong>Input</strong></p><ul><li><code>x</code>: The input to the neural network.</li><li><code>ps</code>: The parameters of the neural network.</li></ul><p><strong>Arguments</strong></p><ul><li><code>model</code>: <code>AbstractExplicitLayer</code>, or a named tuple of them, which will be treated as a <code>Chain</code>.</li><li><code>rng</code>: <code>AbstractRNG</code> to use for initialising the neural network.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/compact/NeuralPDE/pinn_types.jl#L68-L87">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.ConstantFunction" href="#Sophon.ConstantFunction"><code>Sophon.ConstantFunction</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ConstantFunction()</code></pre><p>A conatiner for scalar parameter. This is useful for the case that you want a dummy layer that returns the scalar parameter for any input.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/basic.jl#L290-L295">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.DeepONet" href="#Sophon.DeepONet"><code>Sophon.DeepONet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepONet(branch_net, trunk_net;
         flatten_layer=FlattenLayer(),
         linear_layer=NoOpLayer(),
         bias=ScalarLayer())
DeepONet(layer_sizes_branch, activation_branch,
         layer_sizes_trunk,
         activation_trunk,
         layer_sizes_linear=nothing)</code></pre><p>Deep operator network. Note that the branch net supports multi-dimensional inputs. The <code>flatten_layer</code> flatten the output of the branch net to a matrix, and the <code>linear_layer</code> is applied to the flattened. In this case, <code>linear_layer</code> must be given to transform the flattened matrix to the correct shape.</p><pre><code class="nohighlight hljs">v → branch_net → flatten_layer → linear_layer → b
                                                  ↘
                                                    b&#39; * t + bias → u
                                                  ↗
                                ξ → trunk_net → t</code></pre><p><strong>Arguments</strong></p><ul><li><code>branch_net</code>: The branch net.</li><li><code>trunk_net</code>: The trunk net.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>flatten_layer</code>: The layer to flatten a multi-dimensional array to a matrix.</li><li><code>linear_layer</code>: The layer to apply a linear transformation to the output of the <code>flatten_layer</code>.</li></ul><p><strong>Inputs</strong></p><ul><li><code>(v, ξ)</code>: <code>v</code> is an array of shape <span>$(b_1,b_2,...b_d, m)$</span>, which is a discretization of <span>$m$</span> functions from <span>$R^d$</span> to <span>$R$</span>. <span>$ξ$</span> is a matrix of shape <span>$(d&#39;, n)$</span>, representing <span>$n$</span> data points of the domain <span>$R^{d&#39;}$</span>.</li></ul><p><strong>Returns</strong></p><ul><li>A matrix of shape <span>$(m, n)$</span>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; deeponet = DeepONet((3, 5, 4), relu, (2, 6, 4, 4), tanh)
DeepONet(
    branch_net = Chain(
        layer_1 = Dense(3 =&gt; 5, relu),  # 20 parameters
        layer_2 = Dense(5 =&gt; 4),        # 24 parameters
    ),
    trunk_net = Chain(
        layer_1 = Dense(2 =&gt; 6, tanh_fast),  # 18 parameters
        layer_2 = Dense(6 =&gt; 4, tanh_fast),  # 28 parameters
        layer_3 = Dense(4 =&gt; 4, tanh_fast),  # 20 parameters
    ),
    flatten_layer = FlattenLayer(),
    linear_layer = NoOpLayer(),
    bias = ScalarLayer(),                    # 1 parameters
)         # Total: 111 parameters,
          #        plus 0 states, summarysize 80 bytes.</code></pre><p><strong>Reference</strong></p><p><a href="references/#lu2019deeponet">Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, George Em Karniadakis (2021)</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/operators.jl#L1-L66">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.DiscreteFourierFeature" href="#Sophon.DiscreteFourierFeature"><code>Sophon.DiscreteFourierFeature</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DiscreteFourierFeature(in_dims::Int, out_dims::Int, N::Int, period::Real)</code></pre><p>The discrete Fourier filter proposed in <a href="references/#lindell2021bacon">David B. Lindell, Dave Van Veen, Jeong Joon Park, Gordon Wetzstein (2021)</a>. For a periodic function with period <span>$P$</span>, the Fourier series in amplitude-phase form is</p><p class="math-container">\[s_N(x)=\frac{a_0}{2}+\sum_{n=1}^N{a_n}\cdot \sin \left( \frac{2\pi}{P}nx+\varphi _n \right)\]</p><p>The output is guaranteed to be periodic.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Number of the input dimensions.</li><li><code>out_dims</code>: Number of the output dimensions.</li><li><code>N</code>: <span>$N$</span> in the formula.</li><li><code>period</code>: <span>$P$</span> in the formula.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/basic.jl#L117-L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.FactorizedDense" href="#Sophon.FactorizedDense"><code>Sophon.FactorizedDense</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FactorizedDense(in_dims::Int, out_dims::Int, activation=identity;
                     mean::AbstractFloat=1.0f0, std::AbstractFloat=0.1f0,
                     init_weight=kaiming_uniform(activation), init_bias=zeros32)</code></pre><p>Create a <code>Dense</code> layer where the weight is factorized into twa parts, the scaling factors for each row and the weight matrix.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: number of input dimensions</li><li><code>out_dims</code>: number of output dimensions</li><li><code>activation</code>: activation function</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>mean</code>: mean of the scaling factors</li><li><code>std</code>: standard deviation of the scaling factors</li><li><code>init_weight</code>: weight initialization function</li><li><code>init_bias</code>: bias initialization function</li></ul><p><strong>Input</strong></p><ul><li><code>x</code>: input vector or matrix</li></ul><p><strong>Returns</strong></p><ul><li><code>y = activation.(scale * weight * x+ bias)</code>.</li><li>Empty <code>NamedTuple()</code>.</li></ul><p><strong>Parameters</strong></p><ul><li><code>scale</code>: scaling factors. Shape: <code>(out_dims, 1)</code></li><li><code>weight</code>: Weight Matrix of size <code>(out_dims, in_dims)</code>.</li><li><code>bias</code>: Bias of size <code>(out_dims, 1)</code>.</li></ul><p><strong>References</strong></p><p><a href="references/#wang2022random">Sifan Wang, Hanwen Wang, Jacob H Seidman, Paris Perdikaris (2022)</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/basic.jl#L330-L369">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.FourierFeature" href="#Sophon.FourierFeature"><code>Sophon.FourierFeature</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FourierFeature(in_dims::Int, std::NTuple{N,Pair{S,T}}) where {N,S,T&lt;:Int}
FourierFeature(in_dims::Int, frequencies::NTuple{N, T}) where {N, T &lt;: Real}
FourierFeature(in_dims::Int, out_dims::Int, std::Real)</code></pre><p>Fourier Feature Network.</p><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: Number of the input dimensions.</li><li><code>std</code>: A tuple of pairs of <code>sigma =&gt; out_dims</code>, where <code>sigma</code> is the standard deviation of the Gaussian distribution.</li></ul><p class="math-container">\[\phi^{(i)}(x)=\left[\sin \left(2 \pi W^{(i)} x\right) ; \cos 2 \pi W^{(i)} x\right],\ W^{(i)} \sim \mathcal{N}\left(0, \sigma^{(i)}\right),\ i\in 1, \dots, D\]</p><ul><li><code>frequencies</code>: A tuple of frequencies <span>$(f1,f2,...,fn)$</span>.</li></ul><p class="math-container">\[\phi^{(i)}(x)=\left[\sin \left(2 \pi f_i x\right) ; \cos 2 \pi f_i x\right]\]</p><p><strong>Parameters</strong></p><p>If <code>std</code> is used, then parameters are <code>W</code>s in the formula.</p><p><strong>Inputs</strong></p><ul><li><code>x</code>: <code>AbstractArray</code> with <code>size(x, 1) == in_dims</code>.</li></ul><p><strong>Returns</strong></p><ul><li><span>$[\phi^{(1)}, \phi^{(2)}, ... ,\phi^{(D)}]$</span> with <code>size(y, 1) == sum(last(modes) * 2)</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; f = FourierFeature(2,10,1) # Random Fourier Feature
FourierFeature(2 =&gt; 10)

julia&gt; f = FourierFeature(2, (1 =&gt; 3, 50 =&gt; 4)) # Multi-scale Random Fourier Features
FourierFeature(2 =&gt; 14)

julia&gt;  f = FourierFeature(2, (1,2,3,4)) # Predefined frequencies
FourierFeature(2 =&gt; 16)</code></pre><p><strong>References</strong></p><p><a href="references/#rahimi2007random">Ali Rahimi, Benjamin Recht (2007)</a></p><p><a href="references/#tancik2020fourier">Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, Ren Ng (2020)</a></p><p><a href="references/#wang2021eigenvector">Sifan Wang, Hanwen Wang, Paris Perdikaris (2021)</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/basic.jl#L1-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.NonAdaptiveTraining" href="#Sophon.NonAdaptiveTraining"><code>Sophon.NonAdaptiveTraining</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NonAdaptiveTraining(pde_weights=1, bcs_weights=pde_weights)</code></pre><p>Fixed weights for the loss functions.</p><p><strong>Arguments</strong></p><ul><li><code>pde_weights</code>: weights for the PDE loss functions. If a single number is given, it is used for all PDE loss functions.</li><li><code>bcs_weights</code>: weights for the boundary conditions loss functions. If a single number is given, it is used for all boundary conditions loss functions.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/compact/NeuralPDE/training_strategies.jl#L3-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.PINN" href="#Sophon.PINN"><code>Sophon.PINN</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PINN(chain, rng::AbstractRNG=Random.default_rng())
PINN(rng::AbstractRNG=Random.default_rng(); kwargs...)</code></pre><p>A container for a neural network, its states and its initial parameters. Call <code>gpu</code> and <code>cpu</code> to move the neural network to the GPU and CPU respectively. The default element type of the parameters is <code>Float64</code>.</p><p><strong>Fields</strong></p><ul><li><code>phi</code>: <a href="#Sophon.ChainState"><code>ChainState</code></a> if there is only one neural network, or an named tuple of <a href="#Sophon.ChainState"><code>ChainState</code></a>s if there are multiple neural networks. The names are the same as the dependent variables in the PDE.</li><li><code>init_params</code>: The initial parameters of the neural network.</li></ul><p><strong>Arguments</strong></p><ul><li><code>chain</code>: <code>AbstractExplicitLayer</code> or a named tuple of <code>AbstractExplicitLayer</code>s.</li><li><code>rng</code>: <code>AbstractRNG</code> to use for initialising the neural network. If yout want to set the seed, write</li></ul><pre><code class="language-julia hljs">using Random
rng = Random.default_rng()
Random.seed!(rng, 0)</code></pre><p>and pass <code>rng</code> to <code>PINN</code> as</p><pre><code class="language-julia hljs">using Sophon

chain = FullyConnected((1,6,6,1), sin);

# sinple dependent varibale
pinn = PINN(chain, rng);

# multiple dependent varibales
pinn = PINN(rng;
            a = chain,
            b = chain);</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/compact/NeuralPDE/pinn_types.jl#L1-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.PINNAttention" href="#Sophon.PINNAttention"><code>Sophon.PINNAttention</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PINNAttention(H_net, U_net, V_net, fusion_layers)
PINNAttention(in_dims::Int, out_dims::Int, activation::Function=sin;
              hidden_dims::Int, num_layers::Int)</code></pre><p>The output dimesion of <code>H_net</code> and the input dimension of <code>fusion_layers</code> must be the same. For the second and the third constructor, <code>Dense</code> layers is used for <code>H_net</code>, <code>U_net</code>, and <code>V_net</code>. Note that the first constructer does <strong>not</strong> contain the output layer, but the second one does.</p><pre><code class="nohighlight hljs">                 x → U_net → u                           u
                               ↘                           ↘
x → H_net →  h1 → fusionlayer1 → connection → fusionlayer2 → connection
                               ↗                           ↗
                 x → V_net → v                           v</code></pre><p><strong>Arguments</strong></p><ul><li><code>H_net</code>: <code>AbstractExplicitLayer</code>.</li><li><code>U_net</code>: <code>AbstractExplicitLayer</code>.</li><li><code>V_net</code>: <code>AbstractExplicitLayer</code>.</li><li><code>fusion_layers</code>: <code>Chain</code>.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>num_layers</code>: The number of hidden layers.</li><li><code>hidden_dims</code>: The number of hidden dimensions of each hidden layer.</li></ul><p><strong>Reference</strong></p><p><a href="references/#wang2021understanding">Sifan Wang, Yujun Teng, Paris Perdikaris (2021)</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/nets.jl#L1-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.QuasiRandomSampler" href="#Sophon.QuasiRandomSampler"><code>Sophon.QuasiRandomSampler</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuasiRandomSampler(pde_points, bcs_points=pde_points;
                   sampling_alg=SobolSample())</code></pre><p>Sampler to generate the datasets for PDE and boundary conditions using a quisa-random sampling algorithm. You can call <code>sample(pde, sampler, strategy)</code> on it to generate all the datasets. See <a href="https://github.com/SciML/QuasiMonteCarlo.jl">QuasiMonteCarlo.jl</a> for available sampling algorithms. The default element type of the sampled data is <code>Float64</code>. The initial sampled data lives on GPU if <a href="#Sophon.PINN"><code>PINN</code></a> is. You will need manually move the data to GPU if you want to resample.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/compact/NeuralPDE/pinnsampler.jl#L11-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.RBF" href="#Sophon.RBF"><code>Sophon.RBF</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RBF(in_dims::Int, out_dims::Int, num_centers::Int=out_dims; sigma::AbstractFloat=0.2f0)</code></pre><p>Normalized Radial Basis Fuction Network.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/basic.jl#L223-L227">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.ScalarLayer" href="#Sophon.ScalarLayer"><code>Sophon.ScalarLayer</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ScalarLayer(connection::Function)</code></pre><p>Return connection(scalar, x)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/basic.jl#L272-L276">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.SplitFunction" href="#Sophon.SplitFunction"><code>Sophon.SplitFunction</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SplitFunction(indices...)</code></pre><p>Split the input along the first demision according to indices.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/basic.jl#L306-L310">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.TriplewiseFusion" href="#Sophon.TriplewiseFusion"><code>Sophon.TriplewiseFusion</code></a> — <span class="docstring-category">Type</span></header><section><div><p>TriplewiseFusion(connection, layers...)</p><pre><code class="nohighlight hljs">     u1                    u2
        ↘                     ↘
h1 → layer1 → connection → layer2 → connection
        ↗                     ↗
     v1                    v2</code></pre><p><strong>Arguments</strong></p><ul><li><code>connection</code>: A functio takes 3 inputs and combines them.</li><li><code>layers</code>: <code>AbstractExplicitLayer</code>s or a <code>Chain</code>.</li></ul><p><strong>Inputs</strong></p><p>Layer behaves differently based on input type:</p><ol><li>A tripe of <code>(h, u, v)</code>, where <code>u</code> and <code>v</code> itself are tuples of length <code>N</code>, the <code>layers</code> is also a tuple of length <code>N</code>. The computation is as follows</li></ol><pre><code class="language-julia hljs">for i in 1:N
    h = connection(layers[i](h), u[i], v[i])
end</code></pre><ol><li>A triple of <code>(h, u, v)</code>, where <code>u</code> and <code>v</code> are <code>AbstractArray</code>s.</li></ol><pre><code class="language-julia hljs">for i in 1:N
    h = connection(layers[i](h), u, v)
end</code></pre><p><strong>Parameters</strong></p><ul><li>Parameters of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code></li></ul><p><strong>States</strong></p><ul><li>States of each <code>layer</code> wrapped in a NamedTuple with <code>fields = layer_1, layer_2, ..., layer_N</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/containers.jl#L2-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.BACON-Tuple{Int64, Int64, Int64, Real}" href="#Sophon.BACON-Tuple{Int64, Int64, Int64, Real}"><code>Sophon.BACON</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">BACON(in_dims::Int, out_dims::Int, N::Int, period::Real; hidden_dims::Int, num_layers::Int)</code></pre><p>Band-limited Coordinate Networks (BACON) from <a href="references/#lindell2021bacon">David B. Lindell, Dave Van Veen, Jeong Joon Park, Gordon Wetzstein (2021)</a>. Similar to <a href="#Sophon.FourierFilterNet-Tuple{Int64, Int64}"><code>FourierFilterNet</code></a> but the frequcies are dicrete and nontrainable.</p><p>Tips: It is recommended to set <code>period</code> to be <code>1,2,π</code> or <code>2π</code> for better performance.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/nets.jl#L409-L416">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.FourierAttention-Tuple{Int64, Int64, Function, Any}" href="#Sophon.FourierAttention-Tuple{Int64, Int64, Function, Any}"><code>Sophon.FourierAttention</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">FourierAttention(in_dims::Int, out_dims::Int, activation::Function, std;
                 hidden_dims::Int=512, num_layers::Int=6, modes::NTuple)
FourierAttention(in_dims::Int, out_dims::Int, activation::Function, frequencies;
                 hidden_dims::Int=512, num_layers::Int=6, modes::NTuple)</code></pre><p>A model that combines <a href="#Sophon.FourierFeature"><code>FourierFeature</code></a> and <a href="#Sophon.PINNAttention"><code>PINNAttention</code></a>.</p><pre><code class="nohighlight hljs">x → [FourierFeature(x); x] → PINNAttention</code></pre><p><strong>Arguments</strong></p><ul><li><p><code>in_dims</code>: The input dimension.</p><ul><li><code>out_dims</code>: The output dimension.</li><li><code>activation</code>: The activation function.</li><li><code>std</code>: See <a href="#Sophon.FourierFeature"><code>FourierFeature</code></a>.</li><li><code>frequencies</code>: See <a href="#Sophon.FourierFeature"><code>FourierFeature</code></a>.</li></ul></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>hidden_dim</code>: The hidden dimension of each hidden layer.</li><li><code>num_layers</code>: The number of hidden layers.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; FourierAttention(3, 1, sin, (1 =&gt; 10, 10 =&gt; 10, 50 =&gt; 10); hidden_dims=10, num_layers=3)
Chain(
    layer_1 = SkipConnection(
        FourierFeature(3 =&gt; 60),
        vcat
    ),
    layer_2 = PINNAttention(
        H_net = Dense(63 =&gt; 10, sin),   # 640 parameters
        U_net = Dense(63 =&gt; 10, sin),   # 640 parameters
        V_net = Dense(63 =&gt; 10, sin),   # 640 parameters
        fusion = TriplewiseFusion(
            layers = (layer_1 = Dense(10 =&gt; 10, sin), layer_2 = Dense(10 =&gt; 10, sin), layer_3 = Dense(10 =&gt; 10, sin), layer_4 = Dense(10 =&gt; 10, sin)),  # 440 parameters
        ),
    ),
    layer_3 = Dense(10 =&gt; 1),           # 11 parameters
)         # Total: 2_371 parameters,
          #        plus 90 states, summarysize 192 bytes</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/nets.jl#L74-L121">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.FourierFilterNet-Tuple{Int64, Int64}" href="#Sophon.FourierFilterNet-Tuple{Int64, Int64}"><code>Sophon.FourierFilterNet</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">FourierFilterNet(in_dims::Int, out_dims::Int; hidden_dims::Int, num_layers::Int,
                 bandwidth::Real)</code></pre><p>Multiplicative filter network defined by</p><p class="math-container">\[\begin{aligned}
z^{(1)} &amp;=g\left(x ; \theta^{(1)}\right) \\
z^{(i+1)} &amp;=\left(W^{(i)} z^{(i)}+b^{(i)}\right) \circ \sin \left(\omega^{(i)} x+\phi^{(i)}\right)\right) \\
f(x) &amp;=W^{(k)} z^{(k)}+b^{(k)}
\end{aligned}\]</p><p><strong>Keyword Arguments</strong></p><ul><li><code>bandwidth</code>: The maximum bandwidth of the network. The bandwidth is the sum of each filter&#39;s bandwidth.</li></ul><p><strong>Parameters</strong></p><ul><li>Parameters of the filters:</li></ul><p class="math-container">\[    W\sim \mathcal{U}(-\frac{ω}{n}, \frac{ω}{n}), \quad b\sim \mathcal{U}(-\pi, \pi),\]</p><p>where <code>n</code> is the number of filters.</p><p>For a periodic function with period <span>$P$</span>, the Fourier series in amplitude-phase form is</p><p class="math-container">\[s_N(x)=\frac{a_0}{2}+\sum_{n=1}^N{a_n}\cdot \sin \left( \frac{2\pi}{P}nx+\varphi _n \right)\]</p><p>We have the following relation between the banthwidth and the parameters of the model:</p><p class="math-container">\[ω = 2πB=\frac{2πN}{P}.\]</p><p>where <span>$B$</span> is the bandwidth of the network.</p><p><strong>References</strong></p><p><a href="references/#fathony2021multiplicative">Rizal Fathony, Anit Kumar Sahu, Devin Willmott, J Zico Kolter (2021)</a></p><p><a href="references/#lindell2021bacon">David B. Lindell, Dave Van Veen, Jeong Joon Park, Gordon Wetzstein (2021)</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/nets.jl#L352-L389">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.FourierNet-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function, Tuple{Vararg{T, N}} where {N, T}}} where {N, T&lt;:Int64}" href="#Sophon.FourierNet-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function, Tuple{Vararg{T, N}} where {N, T}}} where {N, T&lt;:Int64}"><code>Sophon.FourierNet</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">FourierNet(ayer_sizes::NTuple, activation, modes::NTuple)</code></pre><p>A model that combines <a href="#Sophon.FourierFeature"><code>FourierFeature</code></a> and <a href="#Sophon.FullyConnected-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function}} where {N, T&lt;:Int64}"><code>FullyConnected</code></a>.</p><pre><code class="nohighlight hljs">x → FourierFeature → FullyConnected → y</code></pre><p><strong>Arguments</strong></p><ul><li><code>in_dims</code>: The number of input dimensions.</li><li><code>layer_sizes</code>: A tuple of hidden dimensions used to construct <code>FullyConnected</code>.</li><li><code>activation</code>: The activation function used to construct <code>FullyConnected</code>.</li><li><code>modes</code>: A tuple of modes used to construct <code>FourierFeature</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; FourierNet((2, 30, 30, 1), sin, (1 =&gt; 10, 10 =&gt; 10, 50 =&gt; 10))
Chain(
    layer_1 = FourierFeature(2 =&gt; 60),
    layer_2 = Dense(60 =&gt; 30, sin),     # 1_830 parameters
    layer_3 = Dense(30 =&gt; 30, sin),     # 930 parameters
    layer_4 = Dense(30 =&gt; 1),           # 31 parameters
)         # Total: 2_791 parameters,
          #        plus 60 states, summarysize 112 bytes.

julia&gt; FourierNet((2, 30, 30, 1), sin, (1, 2, 3, 4))
Chain(
    layer_1 = FourierFeature(2 =&gt; 16),
    layer_2 = Dense(16 =&gt; 30, sin),     # 510 parameters
    layer_3 = Dense(30 =&gt; 30, sin),     # 930 parameters
    layer_4 = Dense(30 =&gt; 1),           # 31 parameters
)         # Total: 1_471 parameters,
          #        plus 4 states, summarysize 96 bytes.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/nets.jl#L132-L169">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.FullyConnected-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function}} where {N, T&lt;:Int64}" href="#Sophon.FullyConnected-Union{Tuple{T}, Tuple{N}, Tuple{Tuple{Vararg{T, N}}, Function}} where {N, T&lt;:Int64}"><code>Sophon.FullyConnected</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">FullyConnected(layer_sizes::NTuple{N, Int}, activation; outermost = true,
               init_weight = kaiming_uniform(activation),
               init_bias = zeros32)
FullyConnected(in_dims::Int, out_dims::Int, activation::Function;
               hidden_dims::Int, num_layers::Int, outermost=true,
               init_weight = kaiming_uniform(activation),
               init_bias = zeros32)</code></pre><p>Create fully connected layers.</p><p><strong>Arguments</strong></p><ul><li><code>layer_sizes</code>: Number of dimensions of each layer.</li><li><code>hidden_dims</code>: Number of hidden dimensions.</li><li><code>num_layers</code>: Number of layers.</li><li><code>activation</code>: Activation function.</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>outermost</code>: Whether to use activation function for the last layer. If <code>false</code>, the activation function is applied to the output of the last layer.</li><li><code>init_weight</code>: Initialization method for the weights.</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; fc = FullyConnected((1, 12, 24, 32), relu)
Chain(
    layer_1 = Dense(1 =&gt; 12, relu),     # 24 parameters
    layer_2 = Dense(12 =&gt; 24, relu),    # 312 parameters
    layer_3 = Dense(24 =&gt; 32),          # 800 parameters
)         # Total: 1_136 parameters,
          #        plus 0 states, summarysize 48 bytes.

julia&gt; fc = FullyConnected(1, 10, relu; hidden_dims=20, num_layers=3)
Chain(
    layer_1 = Dense(1 =&gt; 20, relu),     # 40 parameters
    layer_2 = Dense(20 =&gt; 20, relu),    # 420 parameters
    layer_3 = Dense(20 =&gt; 20, relu),    # 420 parameters
    layer_4 = Dense(20 =&gt; 10),          # 210 parameters
)         # Total: 1_090 parameters,
          #        plus 0 states, summarysize 64 bytes.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/nets.jl#L251-L295">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.Sine-Union{Tuple{Pair{T, T}}, Tuple{T}} where T&lt;:Int64" href="#Sophon.Sine-Union{Tuple{Pair{T, T}}, Tuple{T}} where T&lt;:Int64"><code>Sophon.Sine</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Sine(in_dims::Int, out_dims::Int; omega::Real)</code></pre><p>Sinusoidal layer.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">s = Sine(2, 2; omega=30.0f0) # first layer
s = Sine(2, 2) # hidden layer</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/basic.jl#L195-L206">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.Siren-Tuple{Int64, Int64}" href="#Sophon.Siren-Tuple{Int64, Int64}"><code>Sophon.Siren</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">Siren(in_dims::Int, out_dims::Int; hidden_dims::Int, num_layers::Int, omega=30.0f0,
      init_weight=nothing))
Siren(layer_sizes::Int...; omega=30.0f0, init_weight=nothing)</code></pre><p>Sinusoidal Representation Network.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>omega</code>: The <code>ω₀</code> used for the first layer.</li><li><code>init_weight</code>: The initialization algorithm for the weights of the <strong>input</strong> layer. Note that all hidden layers use <code>kaiming_uniform</code> as the initialization algorithm. The default is<p class="math-container">\[    W\sim \mathcal{U}\left(-\frac{\omega}{fan_{in}}, \frac{\omega}{fan_{in}}\right)\]</p></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; Siren(2, 32, 32, 1; omega=5.0f0)
Chain(
    layer_1 = Dense(2 =&gt; 32, sin),      # 96 parameters
    layer_2 = Dense(32 =&gt; 32, sin),     # 1_056 parameters
    layer_3 = Dense(32 =&gt; 1),           # 33 parameters
)         # Total: 1_185 parameters,
          #        plus 0 states, summarysize 48 bytes.

julia&gt; Siren(3, 1; hidden_dims=20, num_layers=3)
Chain(
    layer_1 = Dense(3 =&gt; 20, sin),      # 80 parameters
    layer_2 = Dense(20 =&gt; 20, sin),     # 420 parameters
    layer_3 = Dense(20 =&gt; 20, sin),     # 420 parameters
    layer_4 = Dense(20 =&gt; 1),           # 21 parameters
)         # Total: 941 parameters,
          #        plus 0 states, summarysize 64 bytes.

# Use your own initialization algorithm for the input layer.
julia&gt; init_weight(rng::AbstractRNG, out_dims::Int, in_dims::Int) = randn(rng, Float32, out_dims, in_dims) .* 2.5f0
julia&gt; chain = Siren(2, 1; num_layers = 4, hidden_dims = 50, init_weight = init_weight)</code></pre><p><strong>Reference</strong></p><p><a href="references/#sitzmann2020implicit">Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, Gordon Wetzstein (2020)</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/layers/nets.jl#L177-L222">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.discretize-Tuple{Any, PINN, Sophon.PINNSampler, Sophon.AbstractTrainingAlg}" href="#Sophon.discretize-Tuple{Any, PINN, Sophon.PINNSampler, Sophon.AbstractTrainingAlg}"><code>Sophon.discretize</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs"> discretize(pde_system::PDESystem, pinn::PINN, sampler::PINNSampler,
                strategy::AbstractTrainingAlg;
                additional_loss)</code></pre><p>Convert the PDESystem into an <code>OptimizationProblem</code>. You can have access to each loss function by calling <code>Sophon.residual_function_1</code>, <code>Sophon.residual_function_2</code>... after calling this function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/compact/NeuralPDE/discretize.jl#L148-L155">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Sophon.gaussian" href="#Sophon.gaussian"><code>Sophon.gaussian</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gaussian(x, a=0.2)</code></pre><p>The Gaussian activation function.</p><p class="math-container">\[e^{\frac{- x^{2}}{2a^{2}}}\]</p><p><strong>Reference</strong></p><p><a href="references/#ramasinghe2021beyond">Sameera Ramasinghe, Simon Lucey (2021)</a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/YichengDWu/Sophon.jl/blob/ca5d01a6ca4cfe871167620e9f45d15ccac17a4b/src/activations.jl#L1-L11">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-nextpage" href="tutorials/discontinuous/">Fitting a nonlinear discontinuous function »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 19 October 2022 05:33">Wednesday 19 October 2022</span>. Using Julia version 1.8.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
